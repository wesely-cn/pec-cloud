# -*- coding:utf-8 -*-
# @FileName  :back_off_queue.py
# @Time      :2025/10/12 17:05
# @Author    :shi lei.wei  <slwei@eppei.com>.

import logging
import random
import threading
import time
from multiprocessing import Queue
from queue import Empty
from typing import Callable, Any, Optional

# åŸºäºPriorityQueueå®ç°çš„ï¼ŒæŒ‡æ•°é€€é¿é‡è¯•ä»»åŠ¡é˜Ÿåˆ—ï¼Œä½†æ˜¯å¤šè¿›ç¨‹ä¸å…±äº«æ•°æ®ï¼Œæ‰€ä»¥æ”¹ä¸ºmultiprocess.Queue
logger = logging.getLogger(__name__)


class ExponentialBackoffQueue:
    """
    ä¸€ä¸ªæ”¯æŒæŒ‡æ•°é€€é¿é‡è¯•çš„ä¼˜å…ˆçº§é˜Ÿåˆ—ã€‚
    ä»»åŠ¡å¤±è´¥åä¼šè‡ªåŠ¨æŒ‰æŒ‡æ•°é€€é¿æ—¶é—´é‡æ–°å…¥é˜Ÿï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚
    """

    def __init__(
        self,
        process_func: Callable[[Any], None],
        max_retries: int = 5,
        base_delay: float = 1.0,
        max_backoff: float = 60.0,
        jitter: bool = True,
        dead_letter_callback: Optional[Callable[[Any, int, Exception], None]] = None,
        worker_count: int = 1
    ):
        """
        :param process_func: å¤„ç†ä»»åŠ¡çš„å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªå‚æ•° data
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆåŒ…å«åˆå§‹å°è¯•ï¼‰
        :param base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        :param max_backoff: æœ€å¤§é€€é¿æ—¶é—´ï¼ˆç§’ï¼‰
        :param jitter: æ˜¯å¦æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆæ¨èå¼€å¯ï¼‰
        :param dead_letter_callback: å½“ä»»åŠ¡è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°æ—¶çš„å›è°ƒå‡½æ•°
        :param worker_count: å¯åŠ¨å¤šå°‘ä¸ªå·¥ä½œçº¿ç¨‹
        """
        self.process_func = process_func
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_backoff = max_backoff
        self.jitter = jitter
        self.dead_letter_callback = dead_letter_callback
        # ä¼˜å…ˆçº§é˜Ÿåˆ—: (next_run_time, data, retry_count)ï¼Œå¤šè¿›ç¨‹ä¸å…±äº«æ•°æ®ï¼æ‰€ä»¥åœ¨å¤šè¿›ç¨‹å¼‚å¸¸
        # self.queue = queue.PriorityQueue()
        self.queue = Queue(maxsize=1024)
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for i in range(worker_count):
            t = threading.Thread(target=self._worker, name=f"BackoffWorker-{i}", daemon=True)
            t.start()
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_t = threading.Thread(target=self._stat_queue, name=f"BackoffWorker-Stat", daemon=True)
        monitor_t.start()
        logger.info(f"ExponentialBackoffQueue å¯åŠ¨ï¼Œ{worker_count} ä¸ªå·¥ä½œçº¿ç¨‹ï¼Œæœ€å¤§é‡è¯•: {max_retries}")

    def add_task(self, data: Any):
        """
        æ·»åŠ æ–°ä»»åŠ¡ï¼ˆåˆå§‹é‡è¯•æ¬¡æ•°ä¸º 0ï¼‰
        """
        # ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´ï¼šç«‹å³æ‰§è¡Œï¼ˆtime.time()ï¼‰
        self.queue.put((time.time(), data, 0))
        logger.debug(f"ğŸ“¥ æ·»åŠ ä»»åŠ¡: {data}")

    def _worker(self):
        """
        å·¥ä½œçº¿ç¨‹ï¼šä»é˜Ÿåˆ—å–å‡ºä»»åŠ¡å¹¶å¤„ç†
        """
        while True:
            try:
                next_time, data, retry_count = self.queue.get(timeout=1)
                # å¦‚æœè¿˜æ²¡åˆ°æ‰§è¡Œæ—¶é—´ï¼Œæ”¾å›é˜Ÿåˆ—
                now = time.time()
                if now < next_time:
                    self.queue.put((next_time, data, retry_count))
                    time.sleep(60)
                    continue
                # æ‰§è¡Œä»»åŠ¡
                try:
                    self.process_func(data)
                    logger.debug(f"âœ… æˆåŠŸå¤„ç†: {data}")
                except Exception as e:
                    retry_count += 1
                    if retry_count < self.max_retries:
                        delay = (2 ** retry_count) * self.base_delay
                        delay = min(delay, self.max_backoff)
                        # æ·»åŠ æŠ–åŠ¨
                        if self.jitter:
                            delay += random.uniform(0, 1)
                        next_run_time = now + delay
                        self.queue.put((next_run_time, data, retry_count))
                        logger.warning(f"ğŸ” {data} ç¬¬ {retry_count} æ¬¡å¤±è´¥ï¼Œ{delay:.2f}s åé‡è¯•")
                    else:
                        logger.error(f"ğŸ’€ {data} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.max_retries}ï¼Œæ”¾å¼ƒ")
                        if self.dead_letter_callback:
                            self.dead_letter_callback(data, retry_count, e)
                # finally:
                #     self.queue.task_done()
            except Empty:
                continue
            except Exception as e:
                logger.exception(f"Worker å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")

    # def join(self):
    #     """ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ"""
    #     self.queue.join()

    def _stat_queue(self):
        """è¿”å›é˜Ÿåˆ—ä¸­ä»»åŠ¡æ•°é‡ï¼ˆè¿‘ä¼¼å€¼ï¼‰"""
        while True:
            try:
                logger.info("backoff queue size: %d", self.queue.qsize())
            except Exception as e:
                logger.exception(e)
            finally:
                time.sleep(60)


# å®šä¹‰ä½ çš„å¤„ç†å‡½æ•°
def my_process_function(data):
    logger.info(f"ğŸ¯ æ­£åœ¨å¤„ç†: {data}")
    if random.random() < 0.8:  # 80% æ¦‚ç‡å¤±è´¥
        raise Exception("æ¨¡æ‹Ÿå¤±è´¥")
    logger.info(f"ğŸ‰ æˆåŠŸ: {data}")


# å®šä¹‰æ­»ä¿¡é˜Ÿåˆ—å›è°ƒ
def on_permanent_failure(data, retry_count, exception):
    logger.exception(f"ğŸš¨ æ°¸ä¹…å¤±è´¥: {data}, é‡è¯•: {retry_count}, é”™è¯¯: {exception}")


if __name__ == "__main__":
    # åˆ›å»ºé˜Ÿåˆ—
    ebq = ExponentialBackoffQueue(
        process_func=my_process_function,
        max_retries=4,
        base_delay=1.0,
        max_backoff=30.0,
        jitter=True,
        dead_letter_callback=on_permanent_failure,
        worker_count=2  # ä¸¤ä¸ªå·¥ä½œçº¿ç¨‹
    )
    # æ·»åŠ ä»»åŠ¡
    for idd in range(3):
        ebq.add_task(f"æ¶ˆæ¯-{idd}")
    # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œï¼ˆå®é™…ä¸­ä½ å¯èƒ½ç”¨å…¶ä»–æ–¹å¼ï¼‰
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("ç¨‹åºé€€å‡º")
